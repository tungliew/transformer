{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a58cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n",
    "# step-by-step annotation of BPE\n",
    "corpus = '''Tokenization is the process of breaking down \n",
    "a sequence of text into smaller units called tokens,\n",
    "which can be words, phrases, or even individual characters.\n",
    "Tokenization is often the first step in natural languages processing tasks \n",
    "such as text classification, named entity recognition, and sentiment analysis.\n",
    "The resulting tokens are typically used as input to further processing steps,\n",
    "such as vectorization, where the tokens are converted\n",
    "into numerical representations for machine learning models to use.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b829062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Tokenization is the process of breaking down \n",
      "a sequence of text into smaller units called tokens,\n",
      "which can be words, phrases, or even individual characters\n",
      "\n",
      "Tokenization is often the first step in natural languages processing tasks \n",
      "such as text classification, named entity recognition, and sentiment analysis\n",
      "\n",
      "The resulting tokens are typically used as input to further processing steps,\n",
      "such as vectorization, where the tokens are converted\n",
      "into numerical representations for machine learning models to use\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split corpus into sentences\n",
    "data = corpus.split(\".\")\n",
    "print(type(data))\n",
    "for item in data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00eb5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 230\n",
    "#bpe_pairs = byte_pair_encoding(data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95bd368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'T o k e n i z a t i o n</w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f</w>': 2, 'b r e a k i n g</w>': 1, 'd o w n</w>': 1, 'a</w>': 1, 's e q u e n c e</w>': 1, 't e x t</w>': 2, 'i n t o</w>': 2, 's m a l l e r</w>': 1, 'u n i t s</w>': 1, 'c a l l e d</w>': 1, 't o k e n s ,</w>': 1, 'w h i c h</w>': 1, 'c a n</w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r</w>': 1, 'e v e n</w>': 1, 'i n d i v i d u a l</w>': 1, 'c h a r a c t e r s</w>': 1, 'o f t e n</w>': 1, 'f i r s t</w>': 1, 's t e p</w>': 1, 'i n</w>': 1, 'n a t u r a l</w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g</w>': 2, 't a s k s</w>': 1, 's u c h</w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n ,</w>': 1, 'n a m e d</w>': 1, 'e n t i t y</w>': 1, 'r e c o g n i t i o n ,</w>': 1, 'a n d</w>': 1, 's e n t i m e n t</w>': 1, 'a n a l y s i s</w>': 1, 'T h e</w>': 1, 'r e s u l t i n g</w>': 1, 't o k e n s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y</w>': 1, 'u s e d</w>': 1, 'i n p u t</w>': 1, 't o</w>': 2, 'f u r t h e r</w>': 1, 's t e p s ,</w>': 1, 'v e c t o r i z a t i o n ,</w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d</w>': 1, 'n u m e r i c a l</w>': 1, 'r e p r e s e n t a t i o n s</w>': 1, 'f o r</w>': 1, 'm a c h i n e</w>': 1, 'l e a r n i n g</w>': 1, 'm o d e l s</w>': 1, 'u s e</w>': 1})\n"
     ]
    }
   ],
   "source": [
    "# iterate the whole process for 230 times\n",
    "# let's illustrate one single process\n",
    "from collections import defaultdict\n",
    "vocab = defaultdict(int)\n",
    "# for every single sentence\n",
    "for line in data:\n",
    "    # split the sentence by blank space\n",
    "    # for every single word in sentence\n",
    "    for word in line.split():\n",
    "        # split word into characters + token\"</w>\"\"\n",
    "        vocab[' '.join(list(word)) + \"</w>\"] +=1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aafe31db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('T', 'o'): 2, ('o', 'k'): 5, ('k', 'e'): 5, ('e', 'n'): 10, ('n', 'i'): 5, ('i', 'z'): 3, ('z', 'a'): 3, ('a', 't'): 6, ('t', 'i'): 9, ('i', 'o'): 6, ('o', 'n</w>'): 2, ('i', 's</w>'): 3, ('t', 'h'): 4, ('h', 'e</w>'): 4, ('p', 'r'): 4, ('r', 'o'): 3, ('o', 'c'): 3, ('c', 'e'): 3, ('e', 's'): 6, ('s', 's</w>'): 1, ('o', 'f</w>'): 2, ('b', 'r'): 1, ('r', 'e'): 5, ('e', 'a'): 2, ('a', 'k'): 1, ('k', 'i'): 1, ('i', 'n'): 10, ('n', 'g</w>'): 5, ('d', 'o'): 1, ('o', 'w'): 1, ('w', 'n</w>'): 1, ('s', 'e'): 5, ('e', 'q'): 1, ('q', 'u'): 1, ('u', 'e'): 1, ('n', 'c'): 1, ('c', 'e</w>'): 1, ('t', 'e'): 7, ('e', 'x'): 2, ('x', 't</w>'): 2, ('n', 't'): 5, ('t', 'o</w>'): 4, ('s', 'm'): 1, ('m', 'a'): 2, ('a', 'l'): 4, ('l', 'l'): 3, ('l', 'e'): 3, ('e', 'r</w>'): 2, ('u', 'n'): 1, ('i', 't'): 3, ('t', 's</w>'): 1, ('c', 'a'): 5, ('e', 'd</w>'): 4, ('t', 'o'): 4, ('n', 's'): 1, ('s', ',</w>'): 4, ('w', 'h'): 2, ('h', 'i'): 2, ('i', 'c'): 4, ('c', 'h</w>'): 3, ('a', 'n</w>'): 1, ('b', 'e</w>'): 1, ('w', 'o'): 1, ('o', 'r'): 2, ('r', 'd'): 1, ('d', 's'): 1, ('p', 'h'): 1, ('h', 'r'): 1, ('r', 'a'): 3, ('a', 's'): 3, ('o', 'r</w>'): 2, ('e', 'v'): 1, ('v', 'e'): 3, ('e', 'n</w>'): 2, ('n', 'd'): 1, ('d', 'i'): 1, ('i', 'v'): 1, ('v', 'i'): 1, ('i', 'd'): 1, ('d', 'u'): 1, ('u', 'a'): 2, ('a', 'l</w>'): 3, ('c', 'h'): 2, ('h', 'a'): 1, ('a', 'r'): 4, ('a', 'c'): 2, ('c', 't'): 2, ('e', 'r'): 4, ('r', 's</w>'): 1, ('o', 'f'): 1, ('f', 't'): 1, ('f', 'i'): 2, ('i', 'r'): 1, ('r', 's'): 1, ('s', 't</w>'): 1, ('s', 't'): 2, ('e', 'p</w>'): 1, ('i', 'n</w>'): 1, ('n', 'a'): 3, ('t', 'u'): 1, ('u', 'r'): 2, ('l', 'a'): 2, ('a', 'n'): 3, ('n', 'g'): 1, ('g', 'u'): 1, ('a', 'g'): 1, ('g', 'e'): 1, ('e', 's</w>'): 1, ('s', 's'): 3, ('s', 'i'): 4, ('t', 'a'): 2, ('s', 'k'): 1, ('k', 's</w>'): 1, ('s', 'u'): 3, ('u', 'c'): 2, ('a', 's</w>'): 3, ('c', 'l'): 1, ('i', 'f'): 1, ('o', 'n'): 5, ('n', ',</w>'): 3, ('a', 'm'): 1, ('m', 'e'): 3, ('t', 'y</w>'): 1, ('e', 'c'): 2, ('c', 'o'): 2, ('o', 'g'): 1, ('g', 'n'): 1, ('n', 'd</w>'): 1, ('i', 'm'): 1, ('n', 't</w>'): 1, ('l', 'y'): 1, ('y', 's'): 1, ('T', 'h'): 1, ('u', 'l'): 1, ('l', 't'): 1, ('n', 's</w>'): 3, ('r', 'e</w>'): 3, ('t', 'y'): 1, ('y', 'p'): 1, ('p', 'i'): 1, ('l', 'y</w>'): 1, ('u', 's'): 2, ('n', 'p'): 1, ('p', 'u'): 1, ('u', 't</w>'): 1, ('f', 'u'): 1, ('r', 't'): 2, ('h', 'e'): 2, ('e', 'p'): 2, ('p', 's'): 1, ('r', 'i'): 2, ('n', 'v'): 1, ('n', 'u'): 1, ('u', 'm'): 1, ('f', 'o'): 1, ('n', 'e</w>'): 1, ('r', 'n'): 1, ('m', 'o'): 1, ('o', 'd'): 1, ('d', 'e'): 1, ('e', 'l'): 1, ('l', 's</w>'): 1, ('s', 'e</w>'): 1})\n"
     ]
    }
   ],
   "source": [
    "# statistics\n",
    "# return the frequency count of pairs of characters\n",
    "pairs = defaultdict(int)\n",
    "for word, freq in vocab.items():\n",
    "    symbols = word.split() # list of characters\n",
    "    for i in range(len(symbols)-1):\n",
    "        pairs[symbols[i],symbols[i+1]] += freq\n",
    "\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b157e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', 'n')\n"
     ]
    }
   ],
   "source": [
    "# choose the paris with the highest frequency\n",
    "best = max(pairs, key=pairs.get)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8031924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T o k en i z a t i o n</w>': 2, 'i s</w>': 2, 't h e</w>': 3, 'p r o c e s s</w>': 1, 'o f</w>': 2, 'b r e a k i n g</w>': 1, 'd o w n</w>': 1, 'a</w>': 1, 's e q u en c e</w>': 1, 't e x t</w>': 2, 'i n t o</w>': 2, 's m a l l e r</w>': 1, 'u n i t s</w>': 1, 'c a l l e d</w>': 1, 't o k en s ,</w>': 1, 'w h i c h</w>': 1, 'c a n</w>': 1, 'b e</w>': 1, 'w o r d s ,</w>': 1, 'p h r a s e s ,</w>': 1, 'o r</w>': 1, 'e v e n</w>': 1, 'i n d i v i d u a l</w>': 1, 'c h a r a c t e r s</w>': 1, 'o f t e n</w>': 1, 'f i r s t</w>': 1, 's t e p</w>': 1, 'i n</w>': 1, 'n a t u r a l</w>': 1, 'l a n g u a g e s</w>': 1, 'p r o c e s s i n g</w>': 2, 't a s k s</w>': 1, 's u c h</w>': 2, 'a s</w>': 3, 'c l a s s i f i c a t i o n ,</w>': 1, 'n a m e d</w>': 1, 'en t i t y</w>': 1, 'r e c o g n i t i o n ,</w>': 1, 'a n d</w>': 1, 's en t i m en t</w>': 1, 'a n a l y s i s</w>': 1, 'T h e</w>': 1, 'r e s u l t i n g</w>': 1, 't o k en s</w>': 2, 'a r e</w>': 2, 't y p i c a l l y</w>': 1, 'u s e d</w>': 1, 'i n p u t</w>': 1, 't o</w>': 2, 'f u r t h e r</w>': 1, 's t e p s ,</w>': 1, 'v e c t o r i z a t i o n ,</w>': 1, 'w h e r e</w>': 1, 'c o n v e r t e d</w>': 1, 'n u m e r i c a l</w>': 1, 'r e p r e s en t a t i o n s</w>': 1, 'f o r</w>': 1, 'm a c h i n e</w>': 1, 'l e a r n i n g</w>': 1, 'm o d e l s</w>': 1, 'u s e</w>': 1}\n"
     ]
    }
   ],
   "source": [
    "#vocab = merge_vocab(best, vocab)\n",
    "import re\n",
    "v_out = {}\n",
    "bigram = re.escape(' '.join(best))\n",
    "p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "for word in vocab:\n",
    "    w_out = p.sub(''.join(best), word)\n",
    "    v_out[w_out] = vocab[word]\n",
    "\n",
    "print(v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
